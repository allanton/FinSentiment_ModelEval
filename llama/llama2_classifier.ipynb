{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1695321579793,"user":{"displayName":"Anton Poletaev","userId":"03808020032477837116"},"user_tz":-60},"id":"84tR1VYtCgRp"},"outputs":[],"source":["# This script uses llama2 to classify the headlines in the test set and store the results.\n","# The script requires significant ammount of RAM and GPU memory to run\n","# and is intended to run in Google Colab (on A100 instance)."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18929,"status":"ok","timestamp":1695322837201,"user":{"displayName":"Anton Poletaev","userId":"03808020032477837116"},"user_tz":-60},"id":"VpANPffoTfoT","outputId":"86d22417-f7c5-4e6d-802c-66782e7dace2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":710,"status":"ok","timestamp":1695322837908,"user":{"displayName":"Anton Poletaev","userId":"03808020032477837116"},"user_tz":-60},"id":"Pmsa0NeWTuP7","outputId":"0d01b35d-acf5-4903-bc4c-f2c172489a64"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Refined_Applied_Project/llama\n"]}],"source":["%cd drive/MyDrive/your_path/Refined_Applied_Project/llama"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76R1IAa-WL-Q"},"outputs":[],"source":["!pip install huggingface_hub langchain transformers accelerate bitsandbytes"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5714,"status":"ok","timestamp":1695322862321,"user":{"displayName":"Anton Poletaev","userId":"03808020032477837116"},"user_tz":-60},"id":"LoRd_KszS9LC"},"outputs":[],"source":["import pandas as pd\n","import transformers\n","import time\n","import torch\n","import gc\n","import configparser\n","import datetime\n","import pytz\n","import os\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from huggingface_hub.hf_api import HfFolder\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCO7ykjeWYWi"},"outputs":[],"source":["# Set the directory path and read in the config file\n","# Get the current working directory\n","script_dir = os.getcwd()\n","abs_config_path = os.path.join(script_dir, '../config.ini')\n","config = configparser.ConfigParser()\n","config.read(abs_config_path)\n","\n","# Set the variables from the config file\n","hf_token = config.get('DEFAULT', 'hf_token') # HuggingFace token for the API\n","HfFolder.save_token(hf_token) # Save the HuggingFace token\n","model_path = config.get('llama2_classifier', 'model')\n","train_size = config.getfloat('DEFAULT', 'train_size')\n","test_df_path = '../data/test_' + str(round(1-train_size,2)) + '.csv'\n","test_df_path = os.path.join(script_dir, test_df_path)\n","few_shot = config.getboolean('DEFAULT', 'few_shot') # Few shot learning\n","\n","# Load the test dataframe\n","df = pd.read_csv(test_df_path)\n","\n","# Load Llama and the respective tokenizer from huggingface\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_path, device_map=\"auto\", load_in_4bit=True,\n","    use_auth_token=True)\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=True,\n","                                          load_in_4_bit=True)\n","\n","pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype='auto',\n","    device_map=\"auto\",\n","    max_new_tokens=10,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    eos_token_id = tokenizer.eos_token_id,\n","    pad_token_id = tokenizer.pad_token_id\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Due to tehcincal issues, the padding side needs to be set to right for the 7B model\n","if model_path == 'meta-llama/Llama-2-7b-chat-hf':\n","  tokenizer.padding_side = \"right\"\n","else:\n","  tokenizer.padding_side = \"left\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1695323614384,"user":{"displayName":"Anton Poletaev","userId":"03808020032477837116"},"user_tz":-60},"id":"_dsdy4ggrZtQ"},"outputs":[],"source":["if few_shot == True:\n","    prompt_template = \"\"\"### Instruction: As a retail investor, you are presented with a financial headline. Your task is to classify the sentiment expressed in the headline using one of the following labels: [NEGATIVE, POSITIVE, NEUTRAL].\n","\n","  # Example 1:\n","  ### Headline: Consolidated pretax profit decreased by 69.2 % to EUR 41.0 mn from EUR 133.1 mn in 2007 .\n","  ### Response: NEGATIVE\n","\n","  # Example 2:\n","  ### Headline: In 2007 , Huhtamaki will continue to invest in organic growth .\n","  ### Response: NEUTRAL\n","\n","  # Example 3:\n","  ### Headline: MD Henning Bahr of Stockmann Gruppen praises the trend , since the chains become stronger and their decision-making processes more clear .\n","  ### Response: POSITIVE\n","\n","  ### Headline: {headline}\n","  ### Response: The sentiment expressed in the headline is\"\"\"\n","\n","else:\n","    prompt_template = \"\"\"### Instruction:\n","    As a retail investor, you are presented with a financial headline. Your task is to classify the sentiment expressed in the headline using one of the following labels: [NEGATIVE, POSITIVE, NEUTRAL].\n","\n","    ### Headline:\n","    {headline}\n","\n","    ### Please respond with only one of the following labels: NEGATIVE, POSITIVE, or NEUTRAL.\n","\n","    ### Response: The sentiment expressed in the headline is\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1695323614385,"user":{"displayName":"Anton Poletaev","userId":"03808020032477837116"},"user_tz":-60},"id":"kUIjCm1TeHPQ"},"outputs":[],"source":["# This inserts the headline into the prompt template\n","# Initialize an empty list to store the formatted strings\n","prompts = []\n","\n","# Iterate over each headline in the DataFrame column and apply the f-string\n","for headline in df['Headline'].tolist():\n","    prompts.append(prompt_template.format(headline = headline))\n","\n","# Apply the pipeline classifier\n","start_time = time.time()\n","raw_responses = pipeline(prompts, batch_size=16)\n","end_time = time.time()\n","\n","elapsed_time = end_time - start_time\n","print(f\"The function took {elapsed_time} seconds to complete.\")\n","\n","torch.cuda.empty_cache()  # Free up memory\n","gc.collect()  # Collect any garbage"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1695323614385,"user":{"displayName":"Anton Poletaev","userId":"03808020032477837116"},"user_tz":-60},"id":"leRE3ZD_d_-F"},"outputs":[],"source":["# Sample code to filter out unwanted characters and retain only the label in the\n","# 'response' variable\n","responses = []\n","unknown_responses = []\n","unknown_count = 0\n","\n","for item in raw_responses:\n","    for sub_item in item:\n","        generated_text = sub_item['generated_text']\n","        response_parts = generated_text.split(\"The sentiment expressed in the headline is\")\n","        if len(response_parts) < 2:\n","            response = 'Empty Response'\n","            unknown_responses.append(response_parts)\n","\n","        else:\n","            response = response_parts[1].strip()\n","            # Check if the response is empty\n","            if not response:\n","                response = 'UNKNOWN'\n","            else:\n","                # Strip away everything but the first word (assuming the first word is the label)\n","                response_list = response.split()\n","                for i in response_list:\n","                  i = i.strip('.,;')\n","                  i = i.upper()\n","                  # print('i upper is ', i)\n","                  for label in [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]:\n","                    if i == label:\n","                      response = i\n","\n","        if response.upper() not in ['NEGATIVE', 'NEUTRAL', 'POSITIVE']:\n","            # Append the list so unrecognised reposnses can be examined later\n","            unknown_responses.append(response)\n","            unknown_count += 1\n","            # This is consistent with Zhang, Yang & Liu (2023)\n","            response = 'NEUTRAL'\n","\n","        responses.append(response)\n","\n","# Add the classified labels to the df\n","df['Predicted_Label'] = responses\n","\n","# Output the number of rows where NEUTRAL was subsituted due to errors/unrecognised output\n","# This is consistent with Zhang, Yang & Liu (2023)\n","print(f\"Number of rows with substituted 'NEUTRAL' in the Predicted_Label column: {unknown_count}.\")\n","\n","# Define a dictionary to map the old values to the new values\n","mapping = {'NEGATIVE': -1, 'NEUTRAL': 0, 'POSITIVE': 1}\n","\n","# Replace the values in the two columns using the mapping dictionary\n","df['True_Label'] = df['True_Label'].map(mapping)\n","df['Predicted_Label'] = df['Predicted_Label'].map(mapping)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(df['True_Label'], df['Predicted_Label'])\n","print(f\"Accuracy: {accuracy}\")\n","# Calculate precision\n","precision = precision_score(df['True_Label'], df['Predicted_Label'], average='weighted')\n","print(f\"Precision: {precision}\")\n","# Calculate recall\n","recall = recall_score(df['True_Label'], df['Predicted_Label'], average='weighted')\n","print(f\"Recall: {recall}\")\n","# Calculate F1 score\n","f1 = f1_score(df['True_Label'], df['Predicted_Label'], average='weighted')\n","print(f\"F1 score: {f1}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1695323614385,"user":{"displayName":"Anton Poletaev","userId":"03808020032477837116"},"user_tz":-60},"id":"cj5t_DIiCgRu"},"outputs":[],"source":["#Store the results in the dataframe\n","df_results_path = os.path.join(script_dir, '../results/df_results.csv')\n","df_results = pd.read_csv(df_results_path)\n","error_count = unknown_count\n","bst = pytz.timezone('Europe/London')\n","now = datetime.datetime.now(bst)\n","formatted_time = now.strftime('%d/%m/%Y/%H:%M')\n","\n","# Storing the results\n","df_results = pd.concat([\n","    df_results,\n","    pd.DataFrame({\n","        'Model': model_path,\n","        'Test_Size': round(1-train_size,2),\n","        'Accuracy': accuracy,\n","        'F1': f1,\n","        'Precision': precision,\n","        'Recall': recall,\n","        'Prompt': prompt_template,\n","        'Error Count': error_count,\n","        'DateTime': formatted_time,\n","        'Few_Shot': few_shot\n","    }, index=[0])\n","], ignore_index=True)\n","\n","df_results.to_csv(df_results_path, index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
